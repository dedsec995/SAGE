{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb21249",
   "metadata": {},
   "source": [
    "### S.A.G.E.\n",
    "\n",
    "Sentiment & \\\n",
    "Agent-based \\\n",
    "Guidance \\\n",
    "Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c3cc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12000bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "LOCAL_OPENAI_KEY = os.getenv(\"LOCAL_OPENAI_KEY\")\n",
    "BASE_URL = os.getenv(\"BASE_URL\")\n",
    "client = OpenAI(api_key = OPENAI_KEY)\n",
    "local_client = OpenAI(api_key=OPENAI_KEY,base_url=\"http://192.168.1.186:11434/v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5d28cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "response = client.responses.parse(\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    input=[\n",
    "        {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        },\n",
    "    ],\n",
    "    text_format=CalendarEvent,\n",
    ")\n",
    "print(response)\n",
    "# event = response.output_parsed\n",
    "# event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af73543a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]\n",
    "\n",
    "completion = local_client.chat.completions.parse(\n",
    "      model=\"gemma3:12b\",\n",
    "      messages=[\n",
    "          {\"role\": \"system\", \"content\": \"Extract the event information.\"},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Alice and Bob are going to a science fair on Friday.\",\n",
    "        },],\n",
    "        response_format=CalendarEvent\n",
    "        )\n",
    "print(completion) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350262dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"name\": \"get_horoscope\",\n",
    "        \"description\": \"Get today's horoscope for an astrological sign.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"sign\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"An astrological sign like Taurus or Aquarius\",\n",
    "                },\n",
    "            },\n",
    "            \"required\": [\"sign\"],\n",
    "        },\n",
    "    },\n",
    "]\n",
    "\n",
    "def get_horoscope(sign):\n",
    "    return f\"{sign}: Next Tuesday you will befriend a baby otter.\"\n",
    "\n",
    "# Create a running input list we will add to over time\n",
    "input_list = [\n",
    "    {\"role\": \"user\", \"content\": \"What is my horoscope? I am an Aquarius.\"}\n",
    "]\n",
    "\n",
    "# 2. Prompt the model with tools defined\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# Save function call outputs for subsequent requests\n",
    "input_list += response.output\n",
    "\n",
    "for item in response.output:\n",
    "    if item.type == \"function_call\":\n",
    "        if item.name == \"get_horoscope\":\n",
    "            # 3. Execute the function logic for get_horoscope\n",
    "            horoscope = get_horoscope(json.loads(item.arguments))\n",
    "            \n",
    "            # 4. Provide function call results to the model\n",
    "            input_list.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\n",
    "                  \"horoscope\": horoscope\n",
    "                })\n",
    "            })\n",
    "\n",
    "print(\"Final input:\")\n",
    "print(input_list)\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-5\",\n",
    "    instructions=\"Respond only with a horoscope generated by a tool.\",\n",
    "    tools=tools,\n",
    "    input=input_list,\n",
    ")\n",
    "\n",
    "# 5. The model should be able to give a response!\n",
    "print(\"Final output:\")\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(\"\\n\" + response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a112b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_url = 'https://www.gutenberg.org/cache/epub/1112/pg1112.txt'\n",
    "filepath = keras.utils.get_file('romeo',data_url)\n",
    "with open(filepath) as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "my_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap = 200,\n",
    "    length_function = len\n",
    ")\n",
    "chunks = my_splitter.split_text(raw_text)\n",
    "\n",
    "def generate_ids(number, size):\n",
    "  import string, random\n",
    "  ids=[]\n",
    "  for i in range(number):\n",
    "    res = ''.join(random.choices(string.ascii_letters, k=size))\n",
    "    ids.append(res)\n",
    "    if len(set(ids)) != i+1:\n",
    "      i-=1\n",
    "      ids.pop(-1)\n",
    "\n",
    "  return ids\n",
    "\n",
    "def get_embeddings(text, model=\"text-embedding-3-small\"):\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    return client.embeddings.create(input=text, model=model).data[0].embedding\n",
    "\n",
    "pre_upsert_df = pd.DataFrame(columns=['id','values','metadata'])\n",
    "\n",
    "def load_chunks(df,split_text):\n",
    "    ids = generate_ids(len(split_text),7)\n",
    "    i = 0\n",
    "    for chunk in split_text:\n",
    "        df.loc[i] = [ids[i],get_embeddings(chunk,model='text-embedding-3-small'), {'text':chunk}]\n",
    "        i+=1\n",
    "    return df\n",
    "\n",
    "my_df = load_chunks(pre_upsert_df,chunks)\n",
    "\n",
    "def prepare_DF(df):\n",
    "  import json,ast\n",
    "  try: df=df.drop('Unnamed: 0',axis=1)\n",
    "  except: print('Unnamed Not Found')\n",
    "  df['values']=df['values'].apply(lambda x: np.array([float(i) for i in x.replace(\"[\",'').replace(\"]\",'').split(',')]))\n",
    "  df['metadata']=df['metadata'].apply(lambda x: ast.literal_eval(x))\n",
    "  return df\n",
    "\n",
    "my_index_df = prepare_DF(my_df)\n",
    "index = pc.Index('my-rag')\n",
    "\n",
    "def convert_data(chunk):\n",
    "    'Converts a pandas dataframe to be a simple list of tuples, formatted how the `upsert()` method in the Pinecone Python client expects.'\n",
    "    data = []\n",
    "    for i in chunk.to_dict('records'):\n",
    "        data.append(i)\n",
    "    return data\n",
    "\n",
    "def load_chunker(seq, size):\n",
    "    'Yields a series of slices of the original iterable, up to the limit of what size is.'\n",
    "    for pos in range(0, len(seq), size):\n",
    "        yield seq.iloc[pos:pos + size]\n",
    "\n",
    "for load_chunk in load_chunker(my_index_df,800):\n",
    "    vectors=convert_data(load_chunk)\n",
    "    index.upsert(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95349adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import uuid\n",
    "from io import BytesIO\n",
    "from urllib.parse import urlparse\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from pypdf import PdfReader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def process_and_upload_to_pinecone(url: str, index_name: str):\n",
    "\n",
    "    try:\n",
    "        pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "        openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        print(\"Successfully connected to Pinecone and OpenAI.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Connection Error: {e}\")\n",
    "        return\n",
    "    print(f\"Downloading data from {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  \n",
    "        parsed_url = urlparse(url)\n",
    "        file_path = parsed_url.path\n",
    "        \n",
    "        raw_text = \"\"\n",
    "        if file_path.lower().endswith('.pdf'):\n",
    "            pdf_file = BytesIO(response.content)\n",
    "            pdf_reader = PdfReader(pdf_file)\n",
    "            print(f\"Parsing PDF file with {len(pdf_reader.pages)} pages...\")\n",
    "            for page in pdf_reader.pages:\n",
    "                raw_text += page.extract_text() if page.extract_text() else \"\"\n",
    "        elif file_path.lower().endswith('.txt'):\n",
    "            print(\"Parsing TXT file...\")\n",
    "            raw_text = response.text\n",
    "        else:\n",
    "            print(f\"Unsupported file type from URL: {url}\")\n",
    "            return\n",
    "            \n",
    "        print(f\"Successfully extracted {len(raw_text)} characters of text.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to download data from URL. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"Splitting text into manageable chunks...\")\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(raw_text)\n",
    "    print(f\"Created {len(chunks)} text chunks.\")\n",
    "\n",
    "    try:\n",
    "        index = pc.Index(index_name)\n",
    "        print(f\"Successfully connected to Pinecone index '{index_name}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not connect to Pinecone index '{index_name}'. Error: {e}\")\n",
    "        return\n",
    "        \n",
    "    batch_size = 100\n",
    "    print(f\"Generating embeddings and uploading to Pinecone in batches of {batch_size}...\")\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch_chunks = chunks[i:i + batch_size]\n",
    "        \n",
    "        try:\n",
    "            response = openai_client.embeddings.create(\n",
    "                model=\"text-embedding-3-small\",\n",
    "                input=[chunk.replace(\"\\n\", \" \") for chunk in batch_chunks]\n",
    "            )\n",
    "            embeddings = [item.embedding for item in response.data]\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to generate embeddings. Error: {e}\")\n",
    "            continue\n",
    "        vectors_to_upsert = []\n",
    "        for j, chunk in enumerate(batch_chunks):\n",
    "            vector_id = str(uuid.uuid4()) \n",
    "            embedding_index = i + j\n",
    "            vectors_to_upsert.append({\n",
    "                \"id\": vector_id,\n",
    "                \"values\": embeddings[j],\n",
    "                \"metadata\": {\"text\": chunk, \"source_url\": url}\n",
    "            })\n",
    "        try:\n",
    "            index.upsert(vectors=vectors_to_upsert)\n",
    "            print(f\"  -> Successfully upserted batch {i//batch_size + 1}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upsert batch. Error: {e}\")\n",
    "\n",
    "    print(\"\\nAll chunks have been processed and uploaded to Pinecone!\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    txt_data_url = 'https://www.gutenberg.org/cache/epub/1112/pg1112.txt'\n",
    "    \n",
    "    pdf_data_url = 'https://arxiv.org/pdf/1706.03762.pdf'\n",
    "    \n",
    "    my_pinecone_index = 'sage'\n",
    "    process_and_upload_to_pinecone(url=pdf_data_url, index_name=my_pinecone_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d985ace2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Downloads data from a URL (PDF or TXT), parses it, creates embeddings,\n",
    "and uploads it to a Pinecone index.\n",
    "\n",
    "Args:\n",
    "url (str): The URL of the .txt or .pdf file.\n",
    "index_name (str): The name of the Pinecone index.\n",
    "\"\"\"    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ddd5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query, embed_model = 'text-embedding-3-small',k=5,index=index):\n",
    "    query_embeddings = get_embeddings(query,model=embed_model)\n",
    "    pinecone_response = index.query(vector=query_embeddings,top_k=k,include_metadata=True)\n",
    "    contexts = [item['metadata']['text'] for item in pinecone_response['matches']] \n",
    "    return contexts, query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae94783",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import fitz  # PyMuPDF\n",
    "from typing import Optional, List, Dict, Any, Generator\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- Helper Functions (from your code, slightly modified) ---\n",
    "\n",
    "def generate_ids(number: int, size: int) -> List[str]:\n",
    "    \"\"\"Generates a list of unique random string IDs.\"\"\"\n",
    "    ids = []\n",
    "    for _ in range(number):\n",
    "        res = ''.join(random.choices(string.ascii_letters, k=size))\n",
    "        while res in ids:  # Ensure uniqueness\n",
    "            res = ''.join(random.choices(string.ascii_letters, k=size))\n",
    "        ids.append(res)\n",
    "    return ids\n",
    "\n",
    "def get_embeddings(text: str, model: str) -> List[float]:\n",
    "    \"\"\"Generates embeddings for a given text using the OpenAI client.\"\"\"\n",
    "    # Assumes 'client' is an initialized OpenAI() client in the global scope\n",
    "    global client\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=text, model=model).data[0].embedding\n",
    "\n",
    "def load_chunks(split_text: List[str], model: str) -> pd.DataFrame:\n",
    "    \"\"\"Creates a DataFrame from text chunks with IDs and embeddings.\"\"\"\n",
    "    df = pd.DataFrame(columns=['id', 'values', 'metadata'])\n",
    "    ids = generate_ids(len(split_text), 7)\n",
    "    for i, chunk in enumerate(split_text):\n",
    "        df.loc[i] = [ids[i], get_embeddings(chunk, model=model), {'text': chunk}]\n",
    "    return df\n",
    "\n",
    "def convert_data(chunk: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Converts a DataFrame chunk to the list-of-dicts format for Pinecone upsert.\"\"\"\n",
    "    data = []\n",
    "    for i in chunk.to_dict('records'):\n",
    "        data.append(i)\n",
    "    return data\n",
    "\n",
    "def load_chunker(seq: pd.DataFrame, size: int) -> Generator[pd.DataFrame, None, None]:\n",
    "    \"\"\"Yields slices of a DataFrame for batch processing.\"\"\"\n",
    "    for pos in range(0, len(seq), size):\n",
    "        yield seq.iloc[pos:pos + size]\n",
    "\n",
    "# --- Main Tool Function ---\n",
    "\n",
    "def embed_and_upload_to_pinecone(\n",
    "    index_name: str,\n",
    "    url: Optional[str] = None,\n",
    "    text: Optional[str] = None,\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 200,\n",
    "    embedding_model: str = \"text-embedding-3-small\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes text from a URL (PDF/TXT) or raw string, chunks it,\n",
    "    creates embeddings, and upserts to a Pinecone index.\n",
    "    \n",
    "    Assumes 'client' (OpenAI) and 'pc' (Pinecone) are initialized globally.\n",
    "    \"\"\"\n",
    "    # Assumes 'pc' is an initialized Pinecone() client in the global scope\n",
    "    global pc\n",
    "    \n",
    "    raw_text = \"\"\n",
    "\n",
    "    # 1. Get Raw Text\n",
    "    if url:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "            if url.lower().endswith('.pdf'):\n",
    "                # Process PDF\n",
    "                with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                    raw_text = \"\".join(page.get_text() for page in doc)\n",
    "            elif url.lower().endswith('.txt'):\n",
    "                # Process TXT\n",
    "                raw_text = response.text\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"message\": \"Unsupported file type. URL must end in .pdf or .txt\"}\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"status\": \"error\", \"message\": f\"Failed to download or access URL: {e}\"}\n",
    "    \n",
    "    elif text:\n",
    "        raw_text = text\n",
    "    \n",
    "    else:\n",
    "        return {\"status\": \"error\", \"message\": \"No input provided. You must specify either a 'url' or 'text'.\"}\n",
    "\n",
    "    if not raw_text:\n",
    "        return {\"status\": \"error\", \"message\": \"Extracted text is empty. Nothing to process.\"}\n",
    "\n",
    "    # 2. Split Text\n",
    "    my_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len\n",
    "    )\n",
    "    chunks = my_splitter.split_text(raw_text)\n",
    "\n",
    "    if not chunks:\n",
    "        return {\"status\": \"error\", \"message\": \"Text splitting resulted in zero chunks.\"}\n",
    "\n",
    "    # 3. Load Chunks into DataFrame with Embeddings\n",
    "    try:\n",
    "        my_df = load_chunks(chunks, model=embedding_model)\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to generate embeddings: {e}\"}\n",
    "\n",
    "    # 4. Connect to Pinecone Index\n",
    "    try:\n",
    "        index = pc.Index(index_name)\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to connect to Pinecone index '{index_name}': {e}\"}\n",
    "\n",
    "    # 5. Upsert in Batches\n",
    "    total_upserted = 0\n",
    "    batch_size = 100  # Pinecone recommends batches of 100\n",
    "    try:\n",
    "        for load_chunk in load_chunker(my_df, batch_size):\n",
    "            vectors = convert_data(load_chunk)\n",
    "            index.upsert(vectors)\n",
    "            total_upserted += len(vectors)\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed during Pinecone upsert: {e}\"}\n",
    "\n",
    "    # 6. Return Success\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"total_chunks_processed\": len(chunks),\n",
    "        \"total_vectors_upserted\": total_upserted,\n",
    "        \"index_name\": index_name\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce66b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import random\n",
    "import requests\n",
    "import pandas as pd\n",
    "import fitz\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "from typing import Optional, List, Dict, Any, Generator\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "OPENAI_KEY = os.getenv(\"OPENAI_KEY\")\n",
    "PINE_KEY = os.getenv('PINE_KEY')\n",
    "pc = Pinecone(api_key=PINE_KEY)\n",
    "client = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "def get_embeddings(text: str, model: str) :\n",
    "    \"\"\"Generates embeddings for a given text using the OpenAI client.\"\"\"\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=text, model=model).data[0].embedding\n",
    "\n",
    "def generate_ids(number: int, size: int) -> List[str]:\n",
    "    ids = []\n",
    "    for _ in range(number):\n",
    "        res = ''.join(random.choices(string.ascii_letters, k=size))\n",
    "        while res in ids:\n",
    "            res = ''.join(random.choices(string.ascii_letters, k=size))\n",
    "        ids.append(res)\n",
    "    return ids\n",
    "\n",
    "def load_chunks(split_text: List[str], model: str) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(columns=['id', 'values', 'metadata'])\n",
    "    ids = generate_ids(len(split_text), 7)\n",
    "    for i, chunk in enumerate(split_text):\n",
    "        df.loc[i] = [ids[i], get_embeddings(chunk, model=model), {'text': chunk}]\n",
    "    return df\n",
    "\n",
    "def convert_data(chunk: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "    return chunk.to_dict('records')\n",
    "\n",
    "def load_chunker(seq: pd.DataFrame, size: int) -> Generator[pd.DataFrame, None, None]:\n",
    "    for pos in range(0, len(seq), size):\n",
    "        yield seq.iloc[pos:pos + size]\n",
    "\n",
    "def embed_and_upload_to_pinecone(\n",
    "    url: Optional[str] = None,\n",
    "    text: Optional[str] = None,\n",
    "    chunk_size: int = 800,\n",
    "    chunk_overlap: int = 200,\n",
    "    embedding_model: str = \"text-embedding-3-small\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Processes text from a URL (PDF/TXT) or raw string, chunks it,\n",
    "    creates embeddings, and upserts to a Pinecone index.\n",
    "    \"\"\"\n",
    "    index = pc.Index('sage')\n",
    "    raw_text = \"\"\n",
    "\n",
    "    if url:\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            if url.lower().endswith('.pdf'):\n",
    "                with fitz.open(stream=response.content, filetype=\"pdf\") as doc:\n",
    "                    raw_text = \"\".join(page.get_text() for page in doc)\n",
    "            elif url.lower().endswith('.txt'):\n",
    "                raw_text = response.text\n",
    "            else:\n",
    "                return {\"status\": \"error\", \"message\": \"Unsupported file type. URL must end in .pdf or .txt\"}\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            return {\"status\": \"error\", \"message\": f\"Failed to download or access URL: {e}\"}\n",
    "    elif text:\n",
    "        raw_text = text\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"message\": \"No input provided. You must specify either 'url' or 'text'.\"}\n",
    "\n",
    "    if not raw_text:\n",
    "        return {\"status\": \"error\", \"message\": \"Extracted text is empty. Nothing to process.\"}\n",
    "\n",
    "    my_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len\n",
    "    )\n",
    "    chunks = my_splitter.split_text(raw_text)\n",
    "    if not chunks:\n",
    "        return {\"status\": \"error\", \"message\": \"Text splitting resulted in zero chunks.\"}\n",
    "\n",
    "    try:\n",
    "        my_df = load_chunks(chunks, model=embedding_model)\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to generate embeddings: {e}\"}\n",
    "\n",
    "    try:\n",
    "        target_index = pc.Index('sage')\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to connect to Pinecone index '{'sage'}': {e}\"}\n",
    "\n",
    "    total_upserted = 0\n",
    "    batch_size = 100\n",
    "    try:\n",
    "        for load_chunk in load_chunker(my_df, batch_size):\n",
    "            vectors = convert_data(load_chunk)\n",
    "            target_index.upsert(vectors)\n",
    "            total_upserted += len(vectors)\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed during Pinecone upsert: {e}\"}\n",
    "\n",
    "    return {\n",
    "        \"status\": \"success\",\n",
    "        \"total_chunks_processed\": len(chunks),\n",
    "        \"total_vectors_upserted\": total_upserted,\n",
    "        \"index_name\": 'sage'\n",
    "    }\n",
    "\n",
    "def get_context(query: str, embed_model: str = 'text-embedding-3-small', k: int = 5) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Retrieves relevant text contexts from the Pinecone index\n",
    "    based on a user's search query.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embeddings = get_embeddings(query, model=embed_model)\n",
    "        pinecone_response = index.query(\n",
    "            vector=query_embeddings, \n",
    "            top_k=k, \n",
    "            include_metadata=True\n",
    "        )\n",
    "        contexts = [item['metadata']['text'] for item in pinecone_response['matches']]\n",
    "        \n",
    "        if not contexts:\n",
    "            return {\"status\": \"success\", \"message\": \"Query successful, but no matching contexts were found.\"}\n",
    "            \n",
    "        return {\"status\": \"success\", \"contexts_found\": contexts}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to retrieve context: {e}\"}\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"embed_and_upload_to_pinecone\",\n",
    "            \"description\": \"Processes text from a URL (PDF/TXT) or raw string, chunks it, creates embeddings, and upserts to Pinecone. One of 'url' or 'text' must be provided.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"url\": {\"type\": \"string\", \"description\": \"The URL of the PDF or TXT file to process. If provided, the 'text' parameter is ignored.\"},\n",
    "                    \"text\": {\"type\": \"string\", \"description\": \"A string of raw text to process. This is used only if the 'url' parameter is not provided.\"},\n",
    "                    \"chunk_size\": {\"type\": \"integer\", \"default\": 800},\n",
    "                    \"chunk_overlap\": {\"type\": \"integer\", \"default\": 200},\n",
    "                    \"embedding_model\": {\"type\": \"string\", \"default\": \"text-embedding-3-small\"}\n",
    "                },\n",
    "                # \"required\": [\"url\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_context\",\n",
    "            \"description\": \"Retrieves relevant text contexts from the Pinecone index based on a user's search query.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"query\": {\"type\": \"string\", \"description\": \"The search query to find relevant context for.\"},\n",
    "                    \"k\": {\"type\": \"integer\", \"default\": 5},\n",
    "                    \"embed_model\": {\"type\": \"string\", \"default\": \"text-embedding-3-small\"}\n",
    "                },\n",
    "                \"required\": [\"query\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "available_tools = {\n",
    "    \"embed_and_upload_to_pinecone\": embed_and_upload_to_pinecone,\n",
    "    \"get_context\": get_context,\n",
    "}\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main loop to run the chat-with-tools.\n",
    "    \"\"\"\n",
    "    print(\"Starting chat... (type 'quit' to exit)\")\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant. You have two tools: one to upload documents to a Pinecone index, and one to retrieve context from it to answer questions.\"}\n",
    "    ]\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_prompt = input(\"You: \")\n",
    "            if user_prompt.lower() == 'quit':\n",
    "                print(\"Ending chat. Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            messages.append({\"role\": \"user\", \"content\": user_prompt})\n",
    "\n",
    "            # --- First API Call: Get model response or tool call ---\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4o\",  # Or your preferred model\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\",\n",
    "            )\n",
    "            response_message = response.choices[0].message\n",
    "            tool_calls = response_message.tool_calls\n",
    "\n",
    "            # --- Check if the model wants to call a tool ---\n",
    "            if tool_calls:\n",
    "                # Append the assistant's request to the message history\n",
    "                messages.append(response_message)\n",
    "                \n",
    "                # --- Execute all tool calls ---\n",
    "                for tool_call in tool_calls:\n",
    "                    function_name = tool_call.function.name\n",
    "                    function_to_call = available_tools.get(function_name)\n",
    "                    \n",
    "                    if not function_to_call:\n",
    "                        print(f\"Error: Model tried to call unknown function '{function_name}'\")\n",
    "                        continue\n",
    "                        \n",
    "                    try:\n",
    "                        # Parse the JSON arguments\n",
    "                        function_args = json.loads(tool_call.function.arguments)\n",
    "                        \n",
    "                        print(f\"--- Calling Tool: {function_name}({function_args}) ---\")\n",
    "                        \n",
    "                        # Call the corresponding Python function\n",
    "                        function_response = function_to_call(**function_args)\n",
    "                        \n",
    "                        print(f\"--- Tool Response: {function_response} ---\")\n",
    "                        \n",
    "                        # Append the tool's output to the message history\n",
    "                        messages.append(\n",
    "                            {\n",
    "                                \"tool_call_id\": tool_call.id,\n",
    "                                \"role\": \"tool\",\n",
    "                                \"name\": function_name,\n",
    "                                \"content\": json.dumps(function_response),  # Convert response to JSON string\n",
    "                            }\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error executing tool {function_name}: {e}\")\n",
    "                        messages.append(\n",
    "                            {\n",
    "                                \"tool_call_id\": tool_call.id,\n",
    "                                \"role\": \"tool\",\n",
    "                                \"name\": function_name,\n",
    "                                \"content\": json.dumps({\"status\": \"error\", \"message\": str(e)}),\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "                final_response = client.chat.completions.create(\n",
    "                    model=\"gpt-4o\",\n",
    "                    messages=messages,\n",
    "                )\n",
    "                final_answer = final_response.choices[0].message.content\n",
    "                print(f\"Assistant: {final_answer}\")\n",
    "                messages.append({\"role\": \"assistant\", \"content\": final_answer})\n",
    "\n",
    "            else:\n",
    "                # --- No tool call, just a direct answer ---\n",
    "                assistant_response = response_message.content\n",
    "                print(f\"Assistant: {assistant_response}\")\n",
    "                messages.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred: {e}\")\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
